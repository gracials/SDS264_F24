---
title: "homework_03"
format:
  pdf: default
editor_options: 
  chunk_output_type: console
---

```{r}
#| include: FALSE

library(tidyverse)
library(stringr)
library(rvest)
library(polite)
library(htmltools)
library(tibble)
library(httr2)
library(httr)
library(janitor)
```

# 07 On Your Own #2-3

2.  Write a function to give choices about year, county, and variables

```{r}
acs_function <- function(year, variables, county){
    tidycensus::get_acs(
    year = year,
    state = "MN",
    geography = "tract",
    variables = variables,
    output = "wide",
    geometry = TRUE,
    county = county, 
    show_call = TRUE
    )
}


acs_function(2021, c("B01003_001E", "B19013_001E"), "Hennepin")
```

3.  Use your function from (2) along with `map` and `list_rbind` to build a data set for Rice county for the years 2019-2021

```{r}

acs_rice <- 2019:2021 |>
  map(acs_function, variables = c("B01003_001E", "B19013_001E"), county = "Rice")

list_rbind(acs_rice)
  
```

# OMDB example

```{r}
#myapikey01 <- Sys.getenv("OMDB_KEY")
#url <- str_c("https://omdbapi.com/?t=Moonlight&y=2016&apikey=", myapikey01)
#I am having trouble getting this code to work using Sys.setenv and Sys.getenv through the Renviron, so I hardcoded my key into the url. 


url <- "https://omdbapi.com/?t=Moonlight&y=2016&apikey=235e12da"

moonlight <- GET(url)
moonlight

details <- content(moonlight, "parse")   
details
```


```{r}
movies <- c("Moonlight", "Little+Women", "Call+Me+By+Your+Name", "Fences", "In+The+Heights")

omdb <- tibble(Title = character(), Awards = character(), Runtime = double(), BoxOffice = double(), Year = double())

# Use for loop to run through API request process 5 times,
#   each time filling the next row in the tibble
#  - can do max of 1000 GETs per day
for(i in 1:5) {
  #url <- str_c("http://www.omdbapi.com/?t=",movies[i],
   #            "&apikey=", myapikey01)
  url <- str_c("http://www.omdbapi.com/?t=",movies[i],
               "&apikey=235e12da")
  Sys.sleep(0.5)
  onemovie <- GET(url)
  details <- content(onemovie, "parse")
  omdb[i,1] <- details$Title
  omdb[i,2] <- details$Awards
  omdb[i,3] <- parse_number(details$Runtime)
  omdb[i,4] <- parse_number(details$BoxOffice)
  omdb[i,5] <- parse_number(details$Year)
}

omdb

```

# 08 On Your Own #2.2-2.4

2) Organize your `rvest` code from (1) into functions from the `polite` package.

```{r}
session <- bow("https://www.hockey-reference.com/teams/MIN/2001.html", force = TRUE)
result <- scrape(session) |>
 html_nodes(css = "table") |> 
  html_table(header = TRUE, fill = TRUE)
player_tibble <- result[[4]]
player_tibble
```


3) Place the code from (2) into a function where the user can input a team and year.  You would then adjust the url accordingly and produce a clean table for the user.

```{r}
hockey_stats <- function(team, year){
  url <- str_c("https://www.hockey-reference.com/teams/", team, "/", year, ".html")
  session <- bow(url, force = TRUE)
  result <- scrape(session) |>
    html_nodes(css = "table") |> 
    html_table(header = TRUE, fill = TRUE)
  player_tibble <- result[[4]] |>
    row_to_names(row_number = 1) |>
    clean_names() |>
    mutate(year = year) |>
    select(player, year, age, pos, gp, pts)
  player_tibble
}

hockey_stats("MIN", 2020)
```


4) Use `map2` and `list_rbind` to build one data set containing Minnesota Wild data from 2001-2004.

```{r}
teams <- rep("MIN", 4)
years <- 2001:2004
temp <- map2(teams, years, hockey_stats)
hockey_data_4yrs <- list_rbind(temp)
hockey_data_4yrs
```

# 09 Pause to Ponder

**[Pause to Ponder:]** Create a function to scrape a single NIH press release page by filling missing pieces labeled `???`:

```{r}

# Helper function to reduce html_nodes() |> html_text() code duplication
get_text_from_page <- function(page, css_selector) {
  page |>
  html_nodes(css_selector) |>
  html_text()
}

# Main function to scrape and tidy desired attributes
scrape_page <- function(url) {
    Sys.sleep(2)
    page <- read_html(url)
    article_titles <- get_text_from_page(page, ".teaser-title")
    article_dates <- get_text_from_page(page, ".date-display-single")
    article_dates <- mdy(article_dates)
    article_description <- get_text_from_page(page, ".teaser-description")
    article_description <- str_trim(str_replace(article_description, 
                                                ".*\\n", 
                                                "")
                                    )
    
    tibble(
      article_titles, article_dates, article_description
    )
}

scrape_page("https://www.nih.gov/news-events/news-releases")
```


**[Pause to Ponder:]** Use a for loop over the first 5 pages:

```{r}

pages <- vector("list", length = 5)

for (i in 1:5) {
    base_url <- "https://www.nih.gov/news-events/news-releases"
    if (i==1) {
        url <- base_url
    } else {
        url <- str_c(base_url, "?page=", i-1)
    }
    pages[[i]] <- scrape_page(url)
}

df_articles <- bind_rows(pages)
head(df_articles)
```


**[Pause to Ponder:]** Use map functions in the purrr package:

```{r}
base_url <- "https://www.nih.gov/news-events/news-releases"
urls_all_pages <- c(base_url, str_c(base_url, "?page=", as.character(1:4)))

pages2 <- purrr::map(urls_all_pages, scrape_page)
df_articles2 <- bind_rows(pages2)
head(df_articles2)
```


